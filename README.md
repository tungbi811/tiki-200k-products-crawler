# Tiki Product Crawler

A high-performance **Python crawler** for collecting large-scale product data from **Tiki.vn**.

This project is designed to download information for **~200,000 products**, store the results as **JSON files**, and optimize data-fetching time using concurrency and batching techniques.

---

## ğŸ“Œ Project Objectives

* Crawl product details from Tiki using public APIs
* Process a large list of product IDs (~200k)
* Store data in multiple `.json` files (â‰ˆ1000 products per file)
* Normalize and clean product descriptions
* Optimize crawling speed and stability

---

## ğŸ“¦ Data Fields Collected

For each product, the crawler collects:

* `id`
* `name`
* `url_key`
* `price`
* `description` (normalized)
* `images` (image URLs)

---

## ğŸ”— Data Sources

* **Product ID list**
  [https://1drv.ms/u/s!AukvlU4z92FZgp4xIlzQ4giHVa5Lpw?e=qDXctn](https://1drv.ms/u/s!AukvlU4z92FZgp4xIlzQ4giHVa5Lpw?e=qDXctn)

* **Product detail API (example)**
  [https://api.tiki.vn/product-detail/api/v1/products/138083218](https://api.tiki.vn/product-detail/api/v1/products/138083218)

> âš ï¸ This project uses publicly accessible endpoints and is intended for **learning and internal data processing purposes only**.

---

## ğŸ— Project Structure

```
tiki-product-crawler/
â”‚
â”œâ”€â”€ crawler/
â”‚   â”œâ”€â”€ fetcher.py        # API requests & retry logic
â”‚   â”œâ”€â”€ parser.py         # Data parsing & normalization
â”‚   â”œâ”€â”€ writer.py         # JSON batching & file writing
â”‚   â””â”€â”€ utils.py          # Helpers (logging, timing, etc.)
â”‚
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run_crawler.py    # Main execution script
â”‚
â”œâ”€â”€ data/                 # Output JSON files (ignored by git)
â”œâ”€â”€ logs/                 # Log files (ignored by git)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Installation

```bash
# Clone repository
git clone https://github.com/your-username/tiki-product-crawler.git
cd tiki-product-crawler

# Create virtual environment
python -m venv .venv
source .venv/bin/activate  # Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

```bash
python scripts/run_crawler.py
```

The crawler will:

1. Load product IDs
2. Fetch product details concurrently
3. Normalize descriptions
4. Save results into JSON files (1000 products per file)

---

## ğŸš€ Performance Optimization

Techniques used to reduce crawling time:

* Concurrent requests (ThreadPool / AsyncIO)
* Request retry & timeout handling
* Batch-based file writing
* Minimal in-memory footprint

Designed to be **scalable and fault-tolerant** for long-running jobs.

---

## ğŸ§¹ Description Normalization

Product descriptions are cleaned by:

* Removing HTML tags
* Normalizing whitespace
* Standardizing encoding (UTF-8)
* Stripping redundant metadata

---

## ğŸ›¡ Notes & Disclaimer

* This project is **not affiliated with Tiki.vn**
* Do **not** use this crawler to violate terms of service
* Recommended for **data engineering practice** only

---

## ğŸ“„ License

MIT License

---

## âœ¨ Future Improvements

* Async HTTP with aiohttp
* Proxy rotation
* Incremental crawling
* Data validation schema
* Export to Parquet / CSV

---

Happy crawling ğŸš€
